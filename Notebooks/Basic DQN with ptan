{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Basic DQN with ptan","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"R7VLDmLzBtWS","colab_type":"code","outputId":"e3e56ef6-f05e-4d98-960a-d77ac769b65e","executionInfo":{"status":"ok","timestamp":1565281358221,"user_tz":-180,"elapsed":7061,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["!pip install ptan"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting ptan\n","  Downloading https://files.pythonhosted.org/packages/41/bc/79b901be607ae861ca24d1ba504ced953c21be05edbd3518a3ff11610932/ptan-0.4.tar.gz\n","Building wheels for collected packages: ptan\n","  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ptan: filename=ptan-0.4-cp36-none-any.whl size=21534 sha256=096088806d640b2a0629196bd209f81b18bda84f26649f18690a61a4cc45ddae\n","  Stored in directory: /root/.cache/pip/wheels/f8/21/fa/ad8d37fd306e72310c8b9b0e24a1bfec36c8587b1721d5c63d\n","Successfully built ptan\n","Installing collected packages: ptan\n","Successfully installed ptan-0.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lG38d0NIMklJ","colab_type":"text"},"source":["#Some utility functions to make our life easier"]},{"cell_type":"code","metadata":{"id":"RczCOkEkCbi7","colab_type":"code","colab":{}},"source":["import sys\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","\n","HYPERPARAMS = {\n","    'pong': {\n","        'env_name':         \"PongNoFrameskip-v4\",\n","        'stop_reward':      18.0,\n","        'run_name':         'pong',\n","        'replay_size':      100000,\n","        'replay_initial':   10000,\n","        'target_net_sync':  1000,\n","        'epsilon_frames':   10**5,\n","        'epsilon_start':    1.0,\n","        'epsilon_final':    0.02,\n","        'learning_rate':    0.0001,\n","        'gamma':            0.99,\n","        'batch_size':       32\n","    }\n","}\n","\n","def unpack_batch(batch):\n","  states, actions, rewards, dones, last_states = [],[],[],[],[]\n","  for exp in batch:\n","    state = np.array(sxp.state, copy=False)\n","    states.append(state)\n","    rewards.append(exp.reward)\n","    actions.append(exp.action)\n","    dones.append(exp.last_state is None)\n","    if exp.last_state is None:\n","      last_states.append(state)\n","      #the result will be masked anyway\n","    else:\n","      last_states.append(np.array(exp.last_state, copy=False))\n","  return np.array(states, copy=False), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(last_states, copy=False)\n","\n","\"\"\"\n","Note how we handle the final transitions in the batch. To avoid the special handling of such cases for terminal \n","transitions we store the initialstate in the last_states array. To make our calculations of the bellman update correct\n","we'll mask such batch entries during the loss calculation using the dones array.\n","\n","The loss function is exactly the same as we had in the previous chapter.\n","\"\"\"\n","\"\"\"\n","****DOUBLE DQN MODIFICATION****\n","\n","  parameter - *double* in the following function\n","  \n","  If double is enabled, we calculate the best action to take in the next state using our main trained network\n","  but values corresponding to this action come from the target network\n","\"\"\"\n","\n","\n","def calc_loss_dqn(batch, net, tgt_net, gamma, device=\"cuda\", double = True):\n","\n","  states, actions, rewards, dones, next_states = unpack_batch(batch)\n","  \n","  states_v = torch.tensor(states).to(device)\n","  next_states_v = torch.tensor(next_states).to(device)\n","  actions_v = torch.tensor(actions).to(device)\n","  rewards_v = torch.tensor(rewards).to(device)\n","  done_mask = torch.ByteTensor(dones).to(device)\n","  \n","  state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","  \n","  #######################################------NOTE MODIFICATION----###################################\n","  if double:\n","    next_state_actions = net(next_states_v).max(1)[1]\n","    next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n","  else:\n","    next_state_values = tgt_net(next_states_v).max(1)[0]\n","  #####################################################################################################\n","  \n","  next_state_values[done_mask] = 0.0\n","  expected_state_action_values = next_state_values.detach()*gamma+rewards_v\n","  return nn.MSELoss()(state_action_values, expected_state_action_values)\n","\n","###############################---------NEW FUNCTION FOR DDQN FUNCTIONALITY-------#####################\n","\"\"\"\n","We just split our held-out states array into equal chunks and pass every chunk to the network to obtain action \n","values. From those values, we choose the action with the largest value and calculate the mean of such values.\n","As our array with states \n","is fixed for the whole training process, and this array is large enough (in the code we store 1000 states),\n","we can compare the dynamics of this mean value in both DQN variants\n","\"\"\"\n","\n","def calc_values_of_states(states, net, device=\"cuda\"):\n","  mean_vals = []\n","  for batch in np.array(states, 64):\n","    states_v = torch.tensor(batch).to(device)\n","    action_values_v = net(states_v)\n","    best_action_values = action_values_v.max(1)[0]\n","  mean_vals.append(best_action_values_v.mean().item())\n","  return np.mean(mean_vals)\n","\n","\n","###########################-------------loss calculation for prioritized buffer with DDQN-------------------###########3\n","\n","def calc_loss_dqn_2(batch, batch_weights, net, tgt_net, gamma, device=\"cuda\", double = True):\n","\n","  states, actions, rewards, dones, next_states = unpack_batch(batch)\n","  \n","  states_v = torch.tensor(states).to(device)\n","  next_states_v = torch.tensor(next_states).to(device)\n","  actions_v = torch.tensor(actions).to(device)\n","  rewards_v = torch.tensor(rewards).to(device)\n","  done_mask = torch.ByteTensor(dones).to(device)\n","  \n","  state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","  \n","  #######################################------NOTE MODIFICATION----###################################\n","  if double:\n","    next_state_actions = net(next_states_v).max(1)[1]\n","    next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n","  else:\n","    next_state_values = tgt_net(next_states_v).max(1)[0]\n","  #####################################################################################################\n","  \n","  next_state_values[done_mask] = 0.0\n","  #For prioritiesed there are no changes up to this point\n","  expected_state_action_values = next_state_values.detach()*gamma+rewards_v\n","  losses_v = batch_weights_v*(state_action_values-expected_state_action_values)**2\n","  return losses_v.mean(), losses_v+1e-5\n","\"\"\"\n","In the last part of the loss calculation we implement the same MSE loss but write our expression explicitly, \n","rather than using the library. This allows us to take into account weights of samples and keep individual loss \n","values for every sample. Those values will be passed tothe priority replay buffer to update priorities. Small values \n","are added to every loss to handle the situation of the loss value, which will lead to zero priority of entry\n","\"\"\"\n","  \n","\"\"\"\n","The following utility class simplifies the training loop\n","\"\"\"\n","\n","class EpsilonTracker:\n","  def __init__(self, epsion_greedy_selector, params):\n","    self.epsilon_greedy_selector = epsilon_greedy_selector\n","    self.epsilon_start = params['epsilon_start']\n","    self.epsilon_final = params['epsilon_final']\n","    self.epsilon_frames = params['epsilon_frames']\n","    self.frame(0)\n","    \n","  def frame(self, frame):\n","    self.epsilon_greedy_selector.epsilon = max(self.epsilon_greedy_final, self.epsilon_start-frame/self.epsilon_frames)\n","    \n","class RewardTracker:\n","  def __init__(self, writer, stop_reward):\n","    self.writer = writer\n","    self.stop_reward = stop_reward\n","    \n","  def __enter__(self):\n","    self.ts = time.time()\n","    self.ts_frame = 0\n","    self.total_rewards = []\n","    return self\n","  \n","  def __exit__(self, *args):\n","    self.writer.close()\n","    \n","  def reward(self, reward, frame, epsilon = None):\n","    self.total_rewards.append(reward)\n","    speed = (frame-self.ts_frame)/(time.time()-self.ts)\n","    self.ts_frame=frame\n","    self.ts = time.time()\n","    mean_reward = np.mean(self.total_rewards[-100:])\n","    epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n","    print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n","            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n","        ))\n","    sys.stdout.flush()\n","    if epsilon is not None:\n","      self.writer.add_scalar(\"epsilon\", epsilon, frame)\n","    self.writer.add_scalar(\"speed\", speed, frame)\n","    self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n","    self.writer.add_scalar(\"reward\", reward, frame)\n","    if mean_reward > self.stop_reward:\n","      print(\"Solved in %d frames!\" % frame)\n","      return True\n","    return False   \n","  \n","  \n","  \"\"\"\n","  \n","  ######################--------REPLAY BUFFER FOR PRIORITIZED REPLAY------------################################\n","  \n","  \n","  \"\"\"\n","  PRIO_REPLAY_ALPHA = 0.6\n","  BETA_START = 0.4\n","  BETA_FRAMES = 100000\n","  \n","  \n","  class PrioReplayBuffer:\n","    def __init__(self, exp_source, buf_size, prob_alpha = 0.6):\n","      self.exp_source_iter = iter(exp_source)\n","      self.prob_alpha = prob_alpha\n","      self.capacity = buf_size\n","      self.pos = 0\n","      self.buffer = []\n","      self.priorities = np.zeros((buf_size,), dtype = np.float32)\n","      \"\"\"\n","      The class for priority replay stores samples in a circular buffer (it allows us to keep a fixed amount of \n","      entries without relocating the list) and NumPy array to keep priorities. We also store the iterator to the experience \n","      source oject, t pull the samples from the environment\n","      \"\"\"\n","      \n","    def __len__(self):\n","      return len(self.buffer)\n","    \n","    def populate(self, count):\n","      max_prio = self.priorities.max() if self.buffer else 1.0\n","      for _ in range(count):\n","        sample = next(self.exp_source_iter)\n","        if len(self.buffer)<self.capacity:\n","          self.buffer.append(sample)\n","        else:\n","          self.buffer[self.pos] = sample\n","        self.priorities[self.pos] = max_prio\n","        self.pos = (self.pos+1)%self.capacity\n","        \n","   \"\"\"\n","   The populate method needs to pull the given amount of transitions from the Experience source object\n","   and store them in the buffer. As our storage for the transitions is implemented as a circular buffer, we have\n","   two different situations with this bufer\n","   \n","    1. When our buffer hasn't reached the maximim capacity, we just need to append a new transition to the buffer\n","    \n","    2. If the buffer is already full, we need to overwrite the oldest transition, which is tracked by the pos field,\n","    and adjust this position module's buffer size\n","   \"\"\"\n","    def sample(self, batch_size, beta=0.4):\n","      if len(self.buffer)==self.capacity:\n","        prios = self.priorities\n","      else:\n","        prios = self.priorities[:self.pos]\n","      probs = prios*self.prob_alpha\n","      probs/=probs.sum()\n","      \n","      indices = np.random.choice(len(self.buffer), batch_size, p = probs)\n","      samples = [self.buffer[idx] for idx in indices]\n","      \n","      total = len(self.buffer)\n","      weights = (total*probs[idicies])**(-beta)\n","      weights /= weights.max()\n","      return samples, indicies, weights\n","    \n","    def update_priorities(self, batch_indicies, batch_priorities):\n","      for idx, prio in zip(batch_indicies, batch_priorities):\n","        self.priorities[ids]=prio\n","    \"\"\"\n","    In the sample method we convert priorities to probabilities using our alpha hyperparameter.\n","    \n","    Then using those probabilities we sample our buffer to obtain a bath of samples\n","    \n","    As the last step we calculate weights for samples in the batch and return three objects\n","    batch, idicies and weights. Indicies for batch samples are required to update priorities for sampled items\n","    \n","    The last function of the priority replay buffer allows us to update new priorities for the processed batch. \n","    It's responsibility of the caller to use this function with the calculated losses for the batch\n","    \"\"\"\n","    \n","    \n","    \n","    ########################################################################################################\n","    ########################################################################################################\n","    ########################################################################################################\n","    \"\"\"\n","                                                CATEGORICAL DQN  \n","    \"\"\"\n","    \n","    def distr_projection(next_distr, rewards, dones, Vmin, Vmax, n_atoms, gamma):\n","      batch_size = len(rewards)\n","      proj_distr = np.zeros((batch_size, n_atoms), dtype=np.float32)\n","      delta_z = (Vmax-Vmin)/(n_atoms-1)\n","      \"\"\"\n","      In the beginning, we allocate the array, that will keep the result of the projection.\n","      This function expects the batch of distributions with a shape (batch_size, n_atoms), array of rewards\n","      flags for completed episodes and our hyperparameters: Vmin, Vmax, n_atoms and gamma. The delta_z variable is \n","      the width of every atom in our value range\n","      \"\"\"\n","      for atom in range(n_atoms):\n","        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards+(Vmin+atom*delta_z)*gamma))\n","        \"\"\"\n","        In the preceding code, we iterate over every atom in the original distribution, that we have and calculate\n","        the place that this atom will be projected by the bellman operator, taking into account our value bounds. For example\n","        the very first atom, with index 0, corresponds with value -10, but for the sample with reward +1 will be projected\n","        into value -10*0.99+1. In other words it will be shifted to the right(assume our gamma is 0.99). If the value falls\n","        beyond our value range given by Vmax and Vmin, we clip it to bounds.\n","        \"\"\"\n","        b_j = (tz_j-Vmin)/delta_z\n","        \"\"\"\n","        In the next line, we calculate the atom numbers that our samples have projected. Of course, samples can be \n","        projected between the atom. In such situations, we'll spread value in the original distribution at the source \n","        atom, between the two atoms that it falls between. this spreading should be carefully handeled, as our target \n","        atom can land exactly at some atom's position. In that case, we just need to add the source distribution value \n","        to the target atom\n","        \"\"\"\n","        l= np.floor(b_j).astype(np.int64)\n","        u = np.ceil(b_j).astype(np.int64)\n","        eq_mask = u==l\n","        proj_distr[eq_mask, l[eq_mask]]+=next_distr[eq_mask, atom]\n","        \"\"\"\n","        The above code handles the situation when the projected atom lands exactly on the target atom. Otherwise, b_j \n","        won't be the integer value and variables l and u (which correspond to the indicies of atoms below and above the\n","        projected point)\n","        \"\"\"\n","        ne_mask = u!=l\n","        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom]*(u-b_j)[ne_mask]\n","        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom]*(b_j-l)[ne_mask]\n","        \"\"\"\n","        When the projected point lands between atoms, we need to spread the probability of the source atom\n","        betweenatoms below and above. This is carried out by the two lines in the above code and, of course, we \n","        need to properly handle the final transitions of episodes. In taht case, our projection shouldn't take into \n","        the account the next distribution and will just have a 1 probability corresponding to the reward obtained. However,\n","        we need, again, to take into the account our atoms and properly distribute the probability if the reward fals between\n","        the atoms. This case is handled by the code branch below, which zeros resulting distribution for samples with the \n","        done flag set and then calculates the resulting projection\n","        \"\"\"\n","        if dones.ny():\n","          proj_distr[dones] = 0.0\n","          tz_j = np.minimum(Vmax, np.maximum(Vmin,rewards[dones]))\n","          b_j = (tz_j-Vmin)/delta_z\n","          l = np.floor(b_j).astype(np.int64)\n","          u = np.ceil(b_j).astype(np.int64)\n","          eq_mask = u==l\n","          eq_dones = dones.copy()\n","          eq_dones[dones] = eq.mask\n","          if eq_dones.any():\n","            proj_distr[eq_dones, 1] = 1.0\n","          ne_mask = u!=l\n","          ne_dones = dones.copy()\n","          ne_dones[dones] = ne_mask\n","          if ne_dones.any():\n","            proj_distr[ne_dones, l] = (u-b_j)[ne_mask]\n","            proj_distr[ne_dones, u] = (b_j-l)[ne_mask]\n","      return proj_distr\n","      \n","def calc_loss_distr(batch, net, tgt_net, gamma, device=\"cuda\", save_prefix=None):\n","  \n","  states, actions, rewards, dones, next_states = unpack_batch(batch)\n","  \n","  batch_size = len(batch)\n","  \n","  states_v = torch.tensor(states).to(device)\n","  next_states_v = torch.tensor(next_states).to(device)\n","  actions_v = torch.tensor(actions).to(device)\n","  \n","  #next state_distribution\n","  next_distr_v, next_qvals_v = tgt_net.both(next_states_v)\n","  next_actions = next_qvals_v.max(1)[1].data.cpu().numpy()\n","  next_distr = tgt_net.apply_softmax(next_distr_v).data.cpu().numpy()\n","\n","  \"\"\"\n","  Later, we'll need both probability distributions and Q-values for the next states, so we use the both() call \n","  to the network, obtain the best actions to take in the next state, apply softmax to the distribution and convert\n","  it to the array\n","  \"\"\"\n","  next_best_distr = next_distr[range(batch_size), next_actions]\n","  dones = dones.astype(np.bool)\n","  proj_distr = distr_projection(next_best_distr, rewards, dones, Vmin, Vmax, N_ATOMS, gamma)\n","  \n","  \"\"\"\n","  Then, we extract distributions of the best actions and perform their projectionusing the bellman opertor. The \n","  result of the projection will be target distribution about what we want our network output to look like\n","  \"\"\"\n","  \n","  distr_v = net(states_v)\n","  state_action_values = distr_v[range(batch_size), actions_v.data]\n","  state_log_sm_v = F.log_softmax(state_action_values, dim=1)\n","  proj_distr_v = torch.tensor(proj_distr).to(device)\n","  loss_v = -state_log_sm_v*proj_distr_v\n","  return loss_v.sum(dim=1).mean()\n","\"\"\"\n","At the end of the function, we need to compute the output of the network and calculate the KL-divergence between\n","projected distribution and the networks output for the taken_actions. KL-divergence shows how much two distibutions\n","differ and is defined D_k_l(P||Q) = -Sum_i(p_i*log(q_i))\n","\n","To calculate the logarithm of probability, we use the PyTorch function log_softmax, which performs both log and \n","softmax in numericaly stable way. The training loop is the same as before with one exception in \n","ptan.DQNAgent creation, which need to use function qvals, instead of the model itself\n","\n","agent = ptan.agent.DQNAgent(lambda x: net.qvals(x), selector, device=device)\n","\"\"\"\n","  \n","          \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6QnawhHNTdU","colab_type":"text"},"source":["#DQN models\n"]},{"cell_type":"code","metadata":{"id":"7tKRz7kvVauZ","colab_type":"code","colab":{}},"source":["!pip install tensorboardX"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9-FBBWNMqfc","colab_type":"code","colab":{}},"source":["import gym\n","import ptan\n","import argparse\n","import torch \n","import torch.optim as optim\n","from tensorboardX import SummaryWriter\n","\n","class DQN(nn.Module):\n","  def __init__(self, input_shape, n_actions):\n","    super(DQN, self).__init__()\n","    \n","    self.conv = nn.Sequential(\n","        nn.Conv2d(input_shape[0], 32, kernel_size = 8, stride = 4),\n","        nn.ReLU(),\n","        nn.Conv2d(32, 64, kernel_size=4, stride = 2),\n","        nn.ReLU(),\n","        nn.Conv2d(64,64, kernel_size=3, stride = 1),\n","        nn.ReLU()\n","    )\n","    \n","    conv_out_size = self._get_conv_out(input_shape)\n","    \n","    self.fc = nn.Sequential(\n","    nn.Linear(conv_out_size, 512),\n","    nn.ReLU(),\n","    nn.Linear(512, n_actions)\n","    )\n","    \n","  def _get_conv_out(self, shape):\n","    o=self.conv(torch.zeros(1, * shape))\n","    return int(np.prod(o.size()))\n","  \n","  def forward(self, x):\n","    fx = x.float()/256\n","    conv_out = self.conv(fx).view(fx.size()[0],-1)\n","    return self.fc(conv_out)\n","  \n","  \n","class NoisyLinear(nn.Linear):\n","  def __init__(self, in_features, out_features, sigma_init=0.017, bias = True):\n","    super(NoisyLinear, self).__init__(in_features, out_features, bias=bisas)\n","    self.sigma_weight = nn.Parameter(torch.full((out_features,in_features), sigma_init))\n","    self.register_buffer(\"epsilon_weight\", torch.zeros(out_features, in_features))\n","    if bias:\n","      self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))\n","      self.register_buffer(\"epsilon_bias\", torch.zeros(out_features))\n","    self.reset_parameters()\n","    \"\"\"\n","    In the constructor we create a matrix for /sigma (values of /nu will be stored in a matrix inherited from \n","    nn.Linear). To make sigmas trainable, we need to wrap the tensor in a nn.Parameter. The register_buffer method\n","    creates a tensor in the network shich won't be updated during backprop, but will be handeled by the nn.Module \n","    machinery(for example, it will be copied to gpu with Cuda() call). An extra parameter and buffer is created \n","    for the bias of the layer. The initial value for sigmas (0.017) was taken from the article. At the end , we will\n","    call the reset parameters method, which was overriden from nn.Linear and is supposed to perform the initializtion \n","    of the layer\n","    \"\"\"\n","    \n","  def reset_parameters(self):\n","    std = math.sqrt(3/self.in_features)\n","    self.weight.data.uniform_(-std, std)\n","    self.bias.data.uniform_(-std,std)\n","    \n","  def forward(self, input):\n","    self.epsilon_weight.normal_()\n","    bias = self.bias\n","    if bias is not None:\n","      self.epsilon_bias.normal_()\n","      bias = bias+ self.sigma_bias*self.epsilon_bias\n","    return F.linear(input, self.weight+self.sigma_weight*self.epsilon_weight, bias)\n","  \"\"\"\n","  In the forward method, we sample random noise in both weight and bias buffers, and perform linear transformation \n","  of the input data in the same way that nn.Linear does. The factorized Gaussian noise works in a similar way and the \n","  author haven't found much difference in the results\n","  \"\"\"\n","class NoisyDQN(nn.Module):\n","  def __init__(self, input_shape, n_actions):\n","    super(NoisyDQN, self).__init__()\n","    \n","    self.conv = nn.Sequential(\n","    nn.Conv2d(input_shape[0], 32, kernel_size=8, stride = 4),\n","        nn.ReLU(),\n","        nn.Conv2d(32, 64, kernel_size = 4, stride =2),\n","        nn.ReLU(),\n","        nn.Conv2d(64,64, kernel_size=3, stride=1)\n","        nn.ReLU()\n","    )\n","    #up to this point it's the same\n","    conv_out_size = self._get_conv_out(input_shape)\n","    self.noisy_layers = [\n","        NoisyLinear(conv_out_size, 512),\n","        NoisyLinear(512, n_actions)\n","    ]\n","    self.fc = nn.Sequential(\n","      self.noisy_layers[0],\n","        nn.ReLU(),\n","        self.noisy_layers[1]\n","    )\n","    \n","    def _get_conv_out(self, shape):\n","      o = self.conv(torch.zeros(1, *shape))\n","      return int(np.prod(o.size()))\n","    \n","    def forward(self, x):\n","      fx = x.float()/256\n","      conv_out = self.conv(fx).view(fx.size()[0], -1)\n","      return self.fc(conv_out)\n","    \n","    #Extra function to calculate the SNR for noisy layers\n","    def noisy_layers_sigma_snr(self):\n","      return[\n","          ((layer.weight**2).mean().sqrt()/(layer.sigma_weight**2).mean().sqrt()).data.cpu().numpy()[0]\n","          for layer in self.noisy_layers\n","      ]\n","    \n","    \"\"\"\n","    After checking SNR chart it can be easily seen that both layers decrease the noise very quickly\n","    But after 250k frames which is roughly the same time as when raw rewards climebed close to the 20 score level\n","    , the level of the noise in the last layer started to increase back, pushing the agent to explore the environment more\n","    This makes alot of sense, as after reaching high score levels, the agent basicaly knows how to play at a good level,\n","    but still needs to polish it's actions to improve the results even more\n","    \"\"\"\n","    \n","    \n","    #######################--------------DUELING DQN---------------------------#######################\n","    \n","class DuelingDQN(nn.Module):\n","  def __init__(self, input_shape, n_actions):\n","    super(NoisyDQN, self).__init__()\n","    \n","    self.conv = nn.Sequential(\n","    nn.Conv2d(input_shape[0], 32, kernel_size=8, stride = 4),\n","        nn.ReLU(),\n","        nn.Conv2d(32, 64, kernel_size = 4, stride =2),\n","        nn.ReLU(),\n","        nn.Conv2d(64,64, kernel_size=3, stride=1)\n","        nn.ReLU()\n","    )\n","    \n","    conv_out_size = self._get_conv_out(input_shape)\n","    self.fc_adv = nn.Sequential(\n","      nn.Linear(conv_out_size, 512),\n","      nn.ReLU()\n","      nn.Linear(512, n_actions)\n","    )\n","    \n","    self.fc_val = nn.Sequential(\n","        nn.Linear(conv_out_size, 512),\n","        nn.ReLU()\n","        nn.Linear(512,1)\n","    )\n","    \n","  def _get_conv_out(self, shape):\n","    o = self.conv(torch.zeros(1, *shape))\n","    return int(np.prod(o.size()))\n","    \n","  def forward(self, x):\n","    fx = x.float()/256\n","    conv_out = self.conv(fx).view(fx.size()[0, -1])\n","    val = self.fc_val(conv_out)\n","    adv = self.fc_adv(conv_out)\n","    return val+adv-adv.mean()\n","  \n","  \n","  ################################################################################################################\n","  ###############################################################################################################3\n","  ################################################################################################################\n","  #####################-----------------------Distributional DQN----------------------############################\n","  \n","  ############################################\n","  SAVE_STATES_IMG = False\n","  SAVE_TRANSITIONS_IMG = False\n","  \n","  if SAVE_STATES_IMG or SAVE_TRANSITIONS_IMG:\n","    import matplotlib as mpl\n","    mpl.use(\"Agg\")\n","    import matplotlib.pylab as plt\n","    \n","  Vmax = 10\n","  Vmin = -10\n","  N_ATOMS = 51\n","  DELTA_Z = (Vmax - Vmin)/(N_ATOMS-1)\n","  \n","  STATES_TO_EVALUATE = 1000\n","  EVAL_EVERY_FRAME = 100\n","  \n","  SAVE_STATES_IMG = False\n","  SAVE_TRANSITIONS_IMG = False\n","  \n","  class DistributionalDQN(nn.Module):\n","    def __init__(self, input_shape, n_actions):\n","      super(DistributionalDQN, self).__init__()\n","      \n","      self.conv = nn.Sequential(\n","          nn.Conv2d(input_shape[0], 32, kernel_size=8, stride = 4),\n","          nn.ReLU(),\n","          nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","          nn.ReLU(),\n","          nn.Conv2d(64,64, kernel_size=3, stride=1),\n","          nn.ReLU()\n","      )\n","      \n","      conv_out_size = self._get_conv_out(input_shape)\n","      self.fc = nn.Sequential(\n","      nn.Linear(conv_out_size, 512),\n","          nn.ReLU(),\n","          nn.Linear(512, n_actions*N_ATOMS)\n","      )\n","      \n","      self.register_buffer(\"supports\", torch.arange(Vmin, Vmax+DELTA_Z, DELTA_Z))\n","      self.softmax = nn.Softmax(dim=1)\n","      \"\"\"\n","      Now it's not the tensor of size n_actions; it's a matrix of n_actions*n_atoms elements, containing probability \n","      distributions for every action. With batch dimenson, the resulting output has three dimensons. We also register \n","      the torch tensor with our atom's values to be able to use it later\n","      \"\"\"\n","      \n","    def _get_conv_out(self, shape):\n","      o = self.conv(torch.zeros(1,*shape[]))\n","      return int(np.prod(o.size()))\n","    \n","    def forward(self, x):\n","      batch_size = x.size()[0]\n","      fx = x.float()/256\n","      conv_out = self.conv(fx).view(batch_size, -1)\n","      fc_out = self.fc(conv_out)\n","      return fc_out.view(batch_size, -1, N_ATOMS)\n","    \n","    \"\"\"\n","    Besides raw distribution, we'll need both distributions and Q-values from the batch of states. To avoid multiple \n","    NN transformation, we'll define the function both(), which returns both raw distribution and q-values. Q-values will \n","    be used to make desicions on actions. Of course, using distributions means that we can have different stratiges for\n","    action selection, but greedy policy in regards to Q-values makes the method comparable to the standart DQN version\n","    \"\"\"\n","    \n","    def both(self, x):\n","      cat_out = self(x)\n","      probs = self.apply_softmax(cat_out)\n","      weights = probs*self.supports\n","      re = weights.sum(dim=2)\n","      return cat_out, res\n","    \"\"\"\n","    To obtain Q-values from the distribution, we just need to calculate the weighted sum of the normalized distribution\n","    and atom's values. the result will be the expected value from the distribution\n","    \"\"\"\n","    \n","    def qvals(self, x):\n","      return self.both(x)[1]\n","    \n","    def apply_softmax(self, t):\n","      return self.softmax(t.view(-1, N_ATOMS)).view(t.size())\n","    \n","    \"\"\"\n","    The remaining functions are just utility functions. The first calculates only Q-values, while the second applies\n","    softmax to the output tensor, keeping the proper shape of tensor\n","    \"\"\"\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LuijWh2Suhb","colab_type":"text"},"source":["#Training loop"]},{"cell_type":"code","metadata":{"id":"j4AZIdrcVX6t","colab_type":"code","outputId":"9173b554-26e1-4e0e-821e-65371ba98479","executionInfo":{"status":"error","timestamp":1565287095805,"user_tz":-180,"elapsed":629,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}},"colab":{"base_uri":"https://localhost:8080/","height":200}},"source":["if __name__ == \"__main__\":\n","  STATES_TO_EVALUATE = 1000\n","  params = HYPERPARAMS['pong']\n","  parser = argparse.ArgumentParser()\n","  parser.add_argument(\"--cuda\", default = True, action = \"store_true\", help = \"Enable cuda\")\n","  parser.add_argument(\"--double\", default=False, action=\"store_true\", help=\"Enable double DQN\")#for DDQN\n","  args = parser.parse_args()\n","  device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","  \n","  env=gym.make(params['env_name'])\n","  env = ptan.common.wrappers.wrap_dqn(env)\n","  \n","  net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n","  tgt_net = ptan.agent.TargetNet(net)\n","  \n","  selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n","  epsilon_tracker = EpsilonTracker(selector, params)\n","  agent = ptan.agent.DQNAgent(net, selector, device = device)\n","  \n","  exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=args.n)\n","  buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n","  \n","  #for prioritized buffer\n","  #buffer = PrioReplayBuffer(exp_source, params['replay_size'], PRIO_REPLAY_ALPHA)\n","  \n","  optimizer = optim.Adam(net.parameters(), lr = params['learning_rate'])\n","  frame_idx = 0\n","  \n","  with RewardTracker(writer, params['stop_reward']) as reward_tracker:\n","    while True:\n","      frame_idx+=1\n","      buffer.populate(1)\n","      epsilon_tracker.frame(frame_idx)\n","      #beta = min(1.0, BETA_START+frame_idx*(1-BETA_START)/BETA_FRAMES)\n","      new_rewards = exp_source.pop_total_rewards()\n","      if new_rewards:\n","        if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n","          break\n","      if len(buffer)< params['replay_initial']:\n","        continue\n","      ###DQN addition###\n","      if eval_states is None:\n","        eval_states = buffer.sample(STATES_TO_EVALUATE)\n","        eval_states = [np.array(transition.state, copy = False) for transition in eval_states]\n","        eval_states = np.array(eval_states, copy=False)\n","      ##################\n","      optimizer.zero_grad()\n","      batch = buffer.sample(params['batch_size'])\n","      #prioritized\n","      #batch, batch_indicies, batch_weights = buffer.sample(params['batch_size'], beta)\n","      #loss_v, sample_prios_v = calc_loss_dqn_2(batch, batch_weights, net, tgt_net.target_model, params['gamma'],\n","      #                                                                        device=device)\n","      #instead of the following line\n","      loss_v = calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params['gamma']**args.n, device = device, double = args.double)\n","      loss_v.backward()\n","      optimizer.step()\n","      #for prioritized\n","      #buffer.update_priorities(batch_indicies, sample_prios_v.data.cpu().numpy())\n","      if frame_idx % params['target_net_sync']==0:\n","        tgt_net.sync()\n","        "],"execution_count":0,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--cuda]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-ca195840-51b9-42e6-b476-ce93e307b6aa.json\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"O-cqOGjxYjyC","colab_type":"text"},"source":["In the beginning of the training loop we create the reward tracker which will report mean reward for every episode completed, increment the frame counter and ask our experience replay buffer to pull one transition from the experience source. This call to buffer.populate(1) will start the following chain of actions inside th ptan lib\n","\n","1. **ExperienceReplayBuffer** will ask the experience source to get the next transition\n","\n","2. The experience source will feed the current observation to the agent to obtain the action\n","\n","3. The agent will aply the nn to the observation to calculate Q-values, then ask the action selector to choose the action to take\n","\n","4. The action selector (which is an epsilon greedy selector) will generate the random number to check how to act: greedily or randomly. In both cases, it will decide which action to take\n","\n","5. The action will be returned to the experience source, which will feed it into the environment to obtain the reward and the next observation. All this data will be returned to buffer\n","\n","6. The buffer will store the transition pushing out old observations to keep its length constant"]},{"cell_type":"code","metadata":{"id":"ASrmbxecX-Rf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}